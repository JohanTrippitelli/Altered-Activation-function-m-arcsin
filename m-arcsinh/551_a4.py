# -*- coding: utf-8 -*-
"""551 - a4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EsZQLdKvyxH9vS0D0lD_dA7NVgGts9vU
"""

# Johan Trippitelli - StudentID: 260917958
# Christopher Chong, StudentID: 260976714
# Minh Anh Trinh, StudentID: 260853143




# Imports from sklearn
import tensorflow as tf
from sklearn.neural_network import MLPClassifier
import numpy as np
from sklearn import svm
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

import time

# Loading Datasets
print("Results for 'Iris' Dataset")
dataset = datasets.load_iris()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)

print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()


print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

"""# New Section"""

# Loading Datasets
print("Results for 'Breast Cancer Wisconsin' Dataset")
dataset = datasets.load_breast_cancer()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# Loading Datasets
print("Results for 'faces' Dataset")
dataset = datasets.fetch_olivetti_faces()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(40, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# Loading Datasets
print("Results for 'wine' Dataset")
dataset = datasets.load_wine()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# Loading Datasets
print("Results for 'digits' Dataset")
dataset = datasets.load_digits()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()
print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# Loading Datasets
print("Results for 'lfw people' Dataset")
#dataset = datasets.fetch_lfw_people()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(5750, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()
print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


for kernel in ("linear", "poly", "rbf", "sigmoid", m_arcsinh):
    classifier = svm.SVC(kernel=kernel, gamma=0.001, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(kernel, accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# gamma chage

# Imports from sklearn
import tensorflow as tf
from sklearn.neural_network import MLPClassifier
import numpy as np
from sklearn import svm
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

import time

# Loading Datasets
print("Results for 'Iris' Dataset")
dataset = datasets.load_iris()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)

print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()


print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM

gamma_values = [0.00001, 0.001, 0.1, 1, 10]
for gamma in gamma_values:
    classifier = svm.SVC(kernel=m_arcsinh, gamma=gamma, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print("gamma = ", gamma)
    print("m_arcsinh", accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

# Loading Datasets
print("Results for 'Breast Cancer Wisconsin' Dataset")
dataset = datasets.load_breast_cancer()

X = dataset.data
y = dataset.target

# Splitting Datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Define the new activation/Kernel function proposed in the article
def m_arcsinh(data, Y):
    return np.dot((
          1/3*np.arcsinh(data))*(1/4*np.sqrt(np.abs(data))),
          (1/3*np.arcsinh(Y.T))*(1/4*np.sqrt(np.abs(Y.T))
          ))

# Updated custom activation function
def m_arcsinh_tf(x):
    return tf.multiply(
        1 / 3 * tf.asinh(x),
        1 / 4 * tf.sqrt(tf.abs(x))
    )

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR MLP")
# MLP (Use custom class for m_arcsinh)
for activation in ('identity', 'logistic', 'tanh', 'relu'):
    classifier = MLPClassifier(activation=activation, random_state=1, max_iter=300)

    start = time.time()
    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print(f"Activation: {activation}, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")

    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
    print()
# Define a neural Net using tensor flow for the new activation function
# Build a simple neural network with a custom activation function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation=m_arcsinh_tf),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
model.fit(X_train, y_train, epochs=300, verbose=0)
stop = time.time()

# Use the model to predict probabilities
y_pred_prob = model.predict(X_test)

# Convert probabilities to class labels
y_pred = tf.argmax(y_pred_prob, axis=1).numpy()

accuracy = accuracy_score(y_test, y_pred)
print(f"Activation: m-arcsinh, Accuracy: {accuracy_score(y_test, y_pred)}, Time: {stop - start}")
print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))
print()

print("----------------------------------------------------------------------------------------------------------------")
print("STARTING TESTING FOR SVM")

# SVM


gamma_values = [0.00001, 0.001, 0.1, 1, 10]
for gamma in gamma_values:
    classifier = svm.SVC(kernel=m_arcsinh, gamma=gamma, random_state=13, class_weight='balanced')
    start = time.time()

    classifier.fit(X_train, y_train)
    stop = time.time()

    y_pred = classifier.predict(X_test)
    print("gamma = ", gamma)
    print("m_arcsinh", accuracy_score(y_test, y_pred))
    print("precision score " , precision_score(y_test, y_pred, average = "weighted"))
    print("recall score " , recall_score(y_test, y_pred, average = "weighted"))
    print("f1 score " , f1_score(y_test, y_pred, average = "weighted"))

    print(f"Training time: {stop - start}s")
    print()

